output_dir: "logs_s1"  # Where checkpoints will be saved
train:
  seed: 1234
  epochs: 100
  precision: 16  # Use half precision for 8GB VRAM
  if_save_latest: true  # Save space by only keeping latest checkpoint
  if_save_every_weights: true
  save_every_n_epoch: 10
  half_weights_save_dir: "weights"  # Directory for saved weights
  exp_name: "2b_gpt"  # Experiment name
  batch_size: 8  # Adjust if you get OOM errors